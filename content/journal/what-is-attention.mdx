---
title: "What's The Deal With Attention?"
date: 2020-12-02
type: draft
description: 'A primer on attention in deep learning.'
---

Attention has proliferated among the state-of-the-art models in deep learning. Also called transformers, attention-based models are now being developed for problems of computer vision (CV), natural language processing (NLP), forecasting, etc.

>> "Our model largely follows the original transformer work."  
>> -- <cite>[Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)</cite>

>> "In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible."  
>> -- <cite>[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)</cite>

>> "TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies."  
>> -- <cite>[Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting](https://arxiv.org/abs/1912.09363)</cite>

Most recently, the AlphaFold model proposed by DeepMind as the state-of-the-art solution to the "protein folding problem" also uses an attention-based model.

>> "For the latest version of AlphaFold, used at CASP14, we created an attention-based neural network system..."  
>> -- <cite>[AlphaFold: a solution to a 50-year-old grand challenge in biology](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)</cite>

Clearly, attention is a big deal. But what exactly is attention? What are transformers? And why have they stirred up so much hype?

## Attention is All You Need

The Transformer model was first introduced in the landmark paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017). It was originally proposed as a novel solution for machine translation, as well as other NLP tasks. 

Prior to the transformer, recurrent neural networks (RNNs) were the de-facto architecture for NLP tasks, successful in modelling dependencies across sequences. However, RNN-based models were limited by their sequential natureâ€”they had difficulty parallalising and learning long-term dependencies. To bypass the limits of sequential computation, the Transformer model uses attention in place of recurrence to model these dependencies.

